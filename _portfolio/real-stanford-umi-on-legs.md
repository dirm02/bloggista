---
layout: project
name: Real Stanford Umi On Legs
slug: real-stanford-umi-on-legs
category: Uncategorized
image: https://raw.githubusercontent.com/dirm02/mystars/master/starred-readmes/real-stanford-umi-on-legs/mani-centric-wbc/docs/assets/umi_on_legs_toss.gif
repo_url: https://github.com/real-stanford/universal_manipulation_interface)
indexed_content: "UMI on Legs: Making Manipulation Policies Mobile with Manipulation-Centric
  Whole-body Controllers [Huy Ha](https://www.cs.columbia.edu/~huy/)$^{\U0001F436,1,2}$,
  [Yihuai Gao](https://yihuai-gao.github.io/)$^{\U0001F436,1}$ [Zipeng Fu](https://zipengfu.github.io/)$^1$,
  [Jie Tan](https://www.jie-tan.net/)$^{3}$ [Shuran Song](https://shurans.github.io/)$^{1,2}$
  $^1$ Stanford University, $^2$ Columbia University, $^3$ Google DeepMind, $^\U0001F436$
  Equal Contribution [Project Page](https://umi-on-legs.github.io/) | [Arxiv](https://arxiv.org/abs/2407.10353)
  | [Video](https://www.youtube.com/watch?v=4Bp0q3xHTxE) UMI on Legs is a framework
  for combining real-world human demonstrations with simulation trained whole-body
  controllers, providing a scalable approach for manipulation skills on robot dogs
  with arms. The best part? You can plug-and-play your existing visuomotor policies
  onto a quadruped, making your manipulation policies mobile! This repository includes
  source code for whole-body controller simulation training, whole-body controller
  real-world deployment, iPhone odometry iOS application, UMI real-world environment
  class, and ARX5 SDK. We've published our code in a similar fashion to how we've
  developed it - as separate submodules - with the hope that the community can easily
  take any component they find useful out and plug it into their own system. If you
  find this codebase useful, consider citing: ```bibtex @inproceedings{ha2024umionlegs,
  title={{UMI} on Legs: Making Manipulation Policies Mobile with Manipulation-Centric
  Whole-body Controllers}, author={Huy Ha and Yihuai Gao and Zipeng Fu and Jie Tan
  and Shuran Song}, year={2024}, } ``` If you have any questions, please contact [Huy
  Ha](https://www.cs.columbia.edu/~huy/) at `huyha [at] stanford [dot] edu` or [Yihuai
  Gao](https://yihuai-gao.github.io/) at `yihuai [at] stanford [dot] edu`. **Table
  of Contents** If you just want to start running some commands while skimming the
  paper, you should [get started here](mani-centric-wbc/docs/starter.md), which downloads
  data, checkpoints, and rolls out the WBC. The rest of the documentation is focused
  on setting up real world deployment. - \U0001F3C3‚Äç‚ôÄÔ∏è [Getting Started](mani-centric-wbc/docs/starter.md)
  - ‚öôÔ∏è [Setup](mani-centric-wbc/docs/starter.md#setup) - \U0001F4CD [Checkpoint &
  Data](mani-centric-wbc/docs/starter.md#downloads) - \U0001F579Ô∏è [Rollout](mani-centric-wbc/docs/starter.md#rollout-controller)
  - \U0001F4CA [Evaluation](mani-centric-wbc/docs/starter.md#evaluation) - \U0001F4C8
  [Curves](https://api.wandb.ai/links/columbia-ai-robotics/rrudtifq) - \U0001F9BE
  [Universal Manipulation Interface](https://github.com/real-stanford/universal_manipulation_interface)
  - \U0001F4F7 [Data Collection](https://swanky-sphere-ad1.notion.site/UMI-Data-Collection-Tutorial-4db1a1f0f2aa4a2e84d9742720428b4c?pvs=4)
  - \U0001F6E0Ô∏è [Hardware Guide](https://docs.google.com/document/d/1TPYwV9sNVPAi0ZlAupDMkXZ4CA1hsZx7YDMSmcEy6EU/edit)
  - \U0001F39BÔ∏è [Preprocessing]([docs/umi/data.md](https://github.com/real-stanford/universal_manipulation_interface?tab=readme-ov-file#running-umi-slam-pipeline))
  - ‚öôÔ∏è [Manipulation-Centric Whole-body Controller](mani-centric-wbc/docs/wbc.md)
  - \U0001F682 [Train](mani-centric-wbc/docs/wbc.md#train) - \U0001F6E1Ô∏è [Robustifying
  Sim2Real](mani-centric-wbc/docs/wbc.md#robustifying-sim2real) - \U0001F52D [Extending](mani-centric-wbc/docs/wbc.md#extending)
  - \U0001F916 [More Robots](mani-centric-wbc/docs/wbc.md#more-robots) - \U0001FAF3
  [More Manipulation Trajectories](mani-centric-wbc/docs/wbc.md#more-manipulation-trajectories)
  - \U0001F30D [Real World Deployment](real-wbc/docs/) - \U0001F914 [Reflections on
  Hardware Choices](real-wbc/docs/hardware_design_choices.md) - \U0001F4DD [Bill of
  Materials](real-wbc/docs/bill_of_materials.md) - \U0001F9BE [ARX5 Robot Arm SDK](https://github.com/yihuai-gao/arx5-sdk)
  - \U0001F4F1 [iPhone Odometry](https://github.com/yihuai-gao/iPhoneVIO) - \U0001F5A8Ô∏è
  [3D Printing Guide](real-wbc/docs/3d_printing.md) - \U0001F6E0Ô∏è [Assembly Guide](real-wbc/docs/assembly.md)
  - \U0001F6DC [Unitree Robots Network Setup](real-wbc/docs/network.md) - \U0001F415
  [Deploy WBC on Real Robots](real-wbc/docs/codebase_setup.md) - \U0001F4FDÔ∏è [Visualizations](mani-centric-wbc/docs/visualization.md)
  # Code Acknowledgements **Whole-body Controller Simulation Training**: - Like many
  other RL for control works nowadays, we started with [Nikita Rudin](https://scholar.google.com/citations?user=1kKJYVIAAAAJ&hl=fr)'s
  implementation of PPO and Gym environment wrapper around IsaacGym, [legged gym](https://github.com/leggedrobotics/legged_gym).
  Shout out to Nikita for publishing such a hackable codebase - it's truly an amazing
  contribution to our community. - Although not used in the final results of the paper,
  our codebase does include a modified Perlin Noise Terrain from [DeepWBC](https://manipulation-locomotion.github.io/).
  To use it, run training with `env.cfg.terrain.mode=perlin`. **Whole-body Controller
  Deployment**: - Thanks to [Qi Wu](https://wooqi57.github.io/) for providing us with
  an initial deployment script for the whole-body controller! **iPhone Odometry Application**:
  - Thanks to [Zhenjia Xu](https://www.zhenjiaxu.com/) for provi"
---
{% raw %}
<h1> UMI on Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers</h1>
<div style="text-align: center;">

[Huy Ha](https://www.cs.columbia.edu/~huy/)$^{üê∂,1,2}$, [Yihuai Gao](https://yihuai-gao.github.io/)$^{üê∂,1}$ [Zipeng Fu](https://zipengfu.github.io/)$^1$, [Jie Tan](https://www.jie-tan.net/)$^{3}$  [Shuran Song](https://shurans.github.io/)$^{1,2}$

$^1$ Stanford University, $^2$ Columbia University, $^3$ Google DeepMind, $^üê∂$ Equal Contribution

[Project Page](https://umi-on-legs.github.io/) | [Arxiv](https://arxiv.org/abs/2407.10353) | [Video](https://www.youtube.com/watch?v=4Bp0q3xHTxE)

<div style="margin:50px; text-align: justify;">
<img style="width:100%;" src="https://raw.githubusercontent.com/dirm02/mystars/master/starred-readmes/real-stanford-umi-on-legs/mani-centric-wbc/docs/assets/umi_on_legs_toss.gif">


UMI on Legs is a framework for combining real-world human demonstrations with simulation trained whole-body controllers, providing a scalable approach for manipulation skills on robot dogs with arms.


<b>The best part?</b> You can plug-and-play your existing visuomotor policies onto a quadruped, making your manipulation policies mobile!


</div>
</div>

<br>

This repository includes source code for whole-body controller simulation training, whole-body controller real-world deployment, iPhone odometry iOS application, UMI real-world environment class, and ARX5 SDK. 
We've published our code in a similar fashion to how we've developed it - as separate submodules - with the hope that the community can easily take any component they find useful out and plug it into their own system.
<!-- TODO: we don't have UMI code yet. Shall we say it here? -->

If you find this codebase useful, consider citing:

```bibtex
@inproceedings{ha2024umionlegs,
      title={{UMI} on Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers}, 
      author={Huy Ha and Yihuai Gao and Zipeng Fu and Jie Tan and Shuran Song},
      year={2024},
}
```

If you have any questions, please contact [Huy Ha](https://www.cs.columbia.edu/~huy/) at `huyha [at] stanford [dot] edu` or [Yihuai Gao](https://yihuai-gao.github.io/) at `yihuai [at] stanford [dot] edu`.

**Table of Contents**

If you just want to start running some commands while skimming the paper, you should [get started here](mani-centric-wbc/docs/starter.md), which downloads data, checkpoints, and rolls out the WBC.
The rest of the documentation is focused on setting up real world deployment.
 
 - üèÉ‚Äç‚ôÄÔ∏è [Getting Started](mani-centric-wbc/docs/starter.md)
   - ‚öôÔ∏è [Setup](mani-centric-wbc/docs/starter.md#setup)
   - üìç [Checkpoint & Data](mani-centric-wbc/docs/starter.md#downloads)
   - üïπÔ∏è [Rollout](mani-centric-wbc/docs/starter.md#rollout-controller)
   - üìä [Evaluation](mani-centric-wbc/docs/starter.md#evaluation)
   - üìà [Curves](https://api.wandb.ai/links/columbia-ai-robotics/rrudtifq)
 - ü¶æ [Universal Manipulation Interface](https://github.com/real-stanford/universal_manipulation_interface)
   - üì∑ [Data Collection](https://swanky-sphere-ad1.notion.site/UMI-Data-Collection-Tutorial-4db1a1f0f2aa4a2e84d9742720428b4c?pvs=4)
   - üõ†Ô∏è [Hardware Guide](https://docs.google.com/document/d/1TPYwV9sNVPAi0ZlAupDMkXZ4CA1hsZx7YDMSmcEy6EU/edit)
   - üéõÔ∏è [Preprocessing]([docs/umi/data.md](https://github.com/real-stanford/universal_manipulation_interface?tab=readme-ov-file#running-umi-slam-pipeline))
 - ‚öôÔ∏è [Manipulation-Centric Whole-body Controller](mani-centric-wbc/docs/wbc.md)
   - üöÇ [Train](mani-centric-wbc/docs/wbc.md#train)
   - üõ°Ô∏è [Robustifying Sim2Real](mani-centric-wbc/docs/wbc.md#robustifying-sim2real)
   - üî≠ [Extending](mani-centric-wbc/docs/wbc.md#extending)
      - ü§ñ [More Robots](mani-centric-wbc/docs/wbc.md#more-robots)
      - ü´≥ [More Manipulation Trajectories](mani-centric-wbc/docs/wbc.md#more-manipulation-trajectories)
 - üåç [Real World Deployment](real-wbc/docs/)
   - ü§î [Reflections on Hardware Choices](real-wbc/docs/hardware_design_choices.md)
   - üìù [Bill of Materials](real-wbc/docs/bill_of_materials.md)
   - ü¶æ [ARX5 Robot Arm SDK](https://github.com/yihuai-gao/arx5-sdk)
   - üì± [iPhone Odometry](https://github.com/yihuai-gao/iPhoneVIO)
   - üñ®Ô∏è [3D Printing Guide](real-wbc/docs/3d_printing.md)
   - üõ†Ô∏è [Assembly Guide](real-wbc/docs/assembly.md)
   - üõú [Unitree Robots Network Setup](real-wbc/docs/network.md)
   - üêï [Deploy WBC on Real Robots](real-wbc/docs/codebase_setup.md)
 - üìΩÔ∏è [Visualizations](mani-centric-wbc/docs/visualization.md)
 

# Code Acknowledgements

**Whole-body Controller Simulation Training**: 
 - Like many other RL for control works nowadays, we started with [Nikita Rudin](https://scholar.google.com/citations?user=1kKJYVIAAAAJ&hl=fr)'s implementation of PPO and Gym environment wrapper around IsaacGym, [legged gym](https://github.com/leggedrobotics/legged_gym). Shout out to Nikita for publishing such a hackable codebase - it's truly an amazing contribution to our community.
 - Although not used in the final results of the paper, our codebase does include a modified Perlin Noise Terrain from [DeepWBC](https://manipulation-locomotion.github.io/). To use it, run training with `env.cfg.terrain.mode=perlin`.

**Whole-body Controller Deployment**: 
 - Thanks to [Qi Wu](https://wooqi57.github.io/) for providing us with an initial deployment script for the whole-body controller!

**iPhone Odometry Application**: 
 - Thanks to [Zhenjia Xu](https://www.zhenjiaxu.com/) for providing us with some starter code for ARKit camera pose publishing!

**UMI Environment Class**:
 - Our UMI deployment codebase heavily builds upon the original [UMI codebase](https://github.com/real-stanford/universal_manipulation_interface). Big thanks to the [UMI team](https://umi-gripper.github.io/)!

**OptiTrack Motion Capture Setup**:
 - Thanks to [Jingyun Yang](https://yjy0625.github.io/) and [Zi-ang Cao](https://github.com/Zi-ang-Cao) for providing the OptiTrack motion capture code and helping us to set it up!
{% endraw %}