---
layout: project
name: Mrkeee Fastumi 100k
slug: MrKeee-FastUMI-100K
category: EE-kicad-3D-Robotic
image: https://img.shields.io/badge/License-MIT-blue.svg
repo_url: https://github.com/dirm02/mystars/tree/master/starred-readmes/MrKeee-FastUMI-100K
indexed_content: "# FastUMI-100K ## Advancing Data-driven Robotic Manipulation with
  a Large-scale UMI-style Dataset [](LICENSE) [](#dataset-features) [](#dataset-features)
  [](#dataset-features) *Large-scale, high-quality robotic manipulation demonstration
  dataset* --- ## \U0001F4D6 Introduction Data-driven robotic manipulation learning
  depends on large-scale, high-quality expert demonstration datasets. However, existing
  datasets, which primarily rely on human teleoperated robot collection, are limited
  in terms of scalability, trajectory smoothness, and applicability across different
  robotic embodiments in real-world environments. We present **FastUMI-100K**, a large-scale
  UMI-style multimodal demonstration dataset, designed to overcome these limitations
  and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI,
  a novel robotic system featuring a modular, hardware-decoupled mechanical design
  and an integrated lightweight tracking system, FastUMI-100K offers a more scalable,
  flexible, and adaptable solution to fulfill the diverse requirements of real-world
  robot demonstration data. --- ## âœ¨ Dataset Features | Feature | Description | |---------|-------------|
  | **\U0001F4CA Scale** | Over 100,000+ demonstration trajectories | | **\U0001F3AF
  Tasks** | Covers 54 tasks and hundreds of object types | | **\U0001F3E0 Environment**
  | Representative household environments | | **\U0001F4F8 Multimodal** | End-effector
  states, multi-view wrist-mounted fisheye images and textual annotations | | **â±ï¸
  Length** | Each trajectory ranges from 120 to 500 frames | --- ## \U0001F680 Coming
  Soon | \U0001F3AF **What's Next** | \U0001F4DD **Description** | |-------------------|-------------------|
  | \U0001F4C4 **Paper** | Detailed technical paper and experimental results | | \U0001F4BE
  **Dataset** | Complete dataset download links | | \U0001F527 **Code** | Source code
  and toolkits | | \U0001F4DA **Documentation** | Comprehensive usage documentation
  and tutorials | --- *More detailed information will be released soon, stay tuned!*
  **â­ If this project helps you, please give us a star!**"
---
{% raw %}
<div align="center">

# FastUMI-100K
## Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Dataset](https://img.shields.io/badge/Dataset-100K%2B%20Trajectories-green.svg)](#dataset-features)
[![Tasks](https://img.shields.io/badge/Tasks-54%20Tasks-orange.svg)](#dataset-features)
[![Objects](https://img.shields.io/badge/Objects-Hundreds%20of%20Types-purple.svg)](#dataset-features)

![FastUMI-100K Overview](https://raw.githubusercontent.com/dirm02/mystars/master/starred-readmes/MrKeee-FastUMI-100K/asset/FastUMI100k.png)

*Large-scale, high-quality robotic manipulation demonstration dataset*

</div>

---

## ğŸ“– Introduction

Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments.

We present **FastUMI-100K**, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data.

---

## âœ¨ Dataset Features

| Feature | Description |
|---------|-------------|
| **ğŸ“Š Scale** | Over 100,000+ demonstration trajectories |
| **ğŸ¯ Tasks** | Covers 54 tasks and hundreds of object types |
| **ğŸ  Environment** | Representative household environments |
| **ğŸ“¸ Multimodal** | End-effector states, multi-view wrist-mounted fisheye images and textual annotations |
| **â±ï¸ Length** | Each trajectory ranges from 120 to 500 frames |

---

## ğŸš€ Coming Soon

<div align="center">

| ğŸ¯ **What's Next** | ğŸ“ **Description** |
|-------------------|-------------------|
| ğŸ“„ **Paper** | Detailed technical paper and experimental results |
| ğŸ’¾ **Dataset** | Complete dataset download links |
| ğŸ”§ **Code** | Source code and toolkits |
| ğŸ“š **Documentation** | Comprehensive usage documentation and tutorials |

</div>

---

<div align="center">

*More detailed information will be released soon, stay tuned!*

**â­ If this project helps you, please give us a star!**

</div>
{% endraw %}